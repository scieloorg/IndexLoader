# Sparkle â€“ Spark OpenAlex â†’ OpenSearch

Projeto Spark unificado para ingestÃ£o de dados do **OpenAlex** no **OpenSearch**, com configuraÃ§Ã£o centralizada via variÃ¡veis de ambiente (ENV) e **com job reutilizÃ¡vel** para mÃºltiplos Ã­ndices.

---

## ğŸ¯ Objetivo

- Eliminar duplicaÃ§Ã£o de scripts Spark  
- Centralizar configuraÃ§Ãµes (OpenSearch, input, Spark)  
- Permitir execuÃ§Ã£o flexÃ­vel por **ENV**  
- Facilitar uso com Docker

---

## ğŸ“ Estrutura do Projeto

```text
sparkle/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py          # ConfiguraÃ§Ãµes centralizadas (ENV)
â”œâ”€â”€ jobs/
â”‚   â””â”€â”€ send_to_opensearch.py    # Job Spark Ãºnico
â”œâ”€â”€ spark/
â”‚   â””â”€â”€ session.py           # CriaÃ§Ã£o da SparkSession
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ opensearch.py        # Escrita no OpenSearch
â””â”€â”€ README.md
```

---

## âš™ï¸ ConfiguraÃ§Ã£o via VariÃ¡veis de Ambiente

Todas as configuraÃ§Ãµes sÃ£o definidas via **ENV** e centralizadas em `config/settings.py`.

### ğŸ“¥ Entrada de dados
```bash
INPUT_DIR=/data
```

### ğŸ” OpenSearch
```bash
OS_INDEX=raw_openalex_publishers
OS_NODES=https://200.136.72.107
OS_PORT=9200
OS_USER=admin
OS_PASS=********
```

### âš¡ Spark
```bash
SPARK_MASTER=local[*]
SPARK_APP_NAME=sparkle-raw_openalex_publishers
```

---

## â–¶ï¸ Exemplos de ExecuÃ§Ã£o

### ğŸ”¹ Publishers
```bash
export INPUT_DIR=/data/publishers
export OS_INDEX=raw_openalex_publishers

spark-submit jobs/send_to_opensearch.py
```

### ğŸ”¹ Works
```bash
export INPUT_DIR=/data/works
export OS_INDEX=raw_openalex_works

spark-submit jobs/send_to_opensearch.py
```

### ğŸ”¹ Institutions
```bash
export INPUT_DIR=/data/institutions
export OS_INDEX=raw_openalex_institutions

spark-submit jobs/send_to_opensearch.py
```

â¡ï¸ O mesmo job atende mÃºltiplos Ã­ndices apenas alterando as variÃ¡veis de ambiente.

---

## ğŸ³ Exemplo com Docker

```bash
docker run --rm \
  -e INPUT_DIR=/data/works \
  -e OS_INDEX=raw_openalex_works \
  -e OS_NODES=https://200.136.72.107 \
  -e OS_USER=admin \
  -e OS_PASS=******** \
  -v $(pwd)/data:/data \
  sparkle \
  spark-submit jobs/send_to_opensearch.py
```

---

## ğŸ§  Funcionamento

1. O Spark lÃª arquivos JSON/JSONL do `INPUT_DIR`
2. O job Spark Ãºnico Ã© executado
3. A escrita no OpenSearch ocorre via conector Spark
4. O Ã­ndice de destino Ã© definido por `OS_INDEX`

---

## ğŸ”’ SeguranÃ§a

âš ï¸ NÃ£o versionar credenciais sensÃ­veis.

Recomenda-se:
- `.env` (nÃ£o versionado)
- Secrets (Docker / Kubernetes)

---

## ğŸ› ï¸ Requisitos

- Apache Spark 3.x  
- Java 11+  
- OpenSearch  
- Conector: `org.opensearch:opensearch-spark-30_2.12`  

---

## ğŸš€ Roadmap

- [ ] Bronze / Silver / Gold  
- [ ] Split por ano (`publication_year`)  
- [ ] RemoÃ§Ã£o de campos pesados (`inverted_abstract`, `fulltext`)    
- [ ] MÃ©tricas e logs estruturados  
## Exemplo para dividir um arquivo grande em partes

## Exemplo para dividir um arquivo grande em partes
```
zcat works-2019.jsonl.gz | split -l 2000000 - works-2019-part-
for f in works-2019-part-*; do gzip -9 "$f"; done
```

## Para baixar a base de dados completa (snapshot) do OpenAlex:

```
aws s3 sync "s3://openalex/data/works" "openalex-snapshot" --no-sign-request
```
Isso farÃ¡ o download de todos os arquivos de trabalhos acadÃªmicos (works) para o diretÃ³rio openalex-snapshot.


## Executar o job
```bash
docker compose run --rm spark-submit \
  spark-submit \
  --master ${SPARK_MASTER_URL:-spark://spark-master:7077} \
  --conf spark.jars.ivy=/tmp/.ivy2 \
  --conf spark.sql.files.maxPartitionBytes=268435456 \
  --conf spark.executor.memory=3g \
  --conf spark.executor.cores=3 \
  --conf spark.executor.instances=4 \
  --packages "${OS_CONNECTOR:-org.opensearch.client:opensearch-spark-30_2.12:1.3.0}" \
  /app/works.py
```
