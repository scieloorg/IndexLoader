
services:
  opensearch:
    image: opensearchproject/opensearch:3.3.2
    container_name: opensearch
    environment:
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - "OPENSEARCH_JAVA_OPTS=-Xms2g -Xmx2g"
      # DEV: sem autenticação/TLS (não use em produção)
      - DISABLE_INSTALL_DEMO_CONFIG=true
      - DISABLE_SECURITY_PLUGIN=true
    ulimits:
      memlock: { soft: -1, hard: -1 }
      nofile:  { soft: 65536, hard: 65536 }
    ports:
      - "9200:9200"
      - "9600:9600"
    volumes:
      - osdata:/usr/share/opensearch/data
    networks: [batchnet]

  dashboards:
    image: opensearchproject/opensearch-dashboards:3.3.0
    container_name: dashboards
    environment:
      - OPENSEARCH_HOSTS=http://opensearch:9200
      # Se você desabilitou segurança no OpenSearch, desabilite também no Dashboards (DEV)
      - DISABLE_SECURITY_DASHBOARDS_PLUGIN=true
    ports:
      - "5601:5601"
    depends_on: [opensearch]
    networks: [batchnet]

  spark-master:
    image: apache/spark:3.5.1
    container_name: spark-master
    environment:
      - SPARK_NO_DAEMONIZE=yes
    command: ["/bin/bash","-lc","/opt/spark/sbin/start-master.sh && tail -f /opt/spark/logs/*"]
    ports:
      - "7077:7077"
      - "8080:8080"
    volumes:
      - ./data:/data:ro          # << montar dados aqui também
    networks: [batchnet]

  spark-worker-1:
    image: apache/spark:3.5.1
    container_name: spark-worker-1
    environment:
      - SPARK_WORKER_CORES=${SPARK_WORKER_CORES:-4}
      - SPARK_WORKER_MEMORY=${SPARK_WORKER_MEMORY:-8G}
    depends_on: [spark-master]
    command: ["/bin/bash","-lc","/opt/spark/sbin/start-worker.sh spark://spark-master:7077 && tail -f /opt/spark/logs/*"]
    ports:
      - "8081:8081"
    volumes:
      - ./data:/data:ro          
    networks: [batchnet]

  spark-worker-2:
    image: apache/spark:3.5.1
    container_name: spark-worker-2
    environment:
      - SPARK_WORKER_CORES=${SPARK_WORKER_CORES:-4}
      - SPARK_WORKER_MEMORY=${SPARK_WORKER_MEMORY:-8G}
    depends_on: [spark-master]
    command: ["/bin/bash","-lc","/opt/spark/sbin/start-worker.sh spark://spark-master:7077 && tail -f /opt/spark/logs/*"]
    ports:
      - "8082:8082"
    volumes:
      - ./data:/data:ro          
    networks: [batchnet]

  spark-submit:
    image: apache/spark:3.5.1
    container_name: spark-submit
    working_dir: /app
    command: ["/bin/bash","-lc","sleep infinity"]
    environment:
      - PATH=/opt/spark/bin:/opt/spark/sbin:$PATH
      - SPARK_MASTER_URL=spark://spark-master:7077
      - OS_CONNECTOR=org.opensearch.client:opensearch-spark-30_2.12:1.3.0
    volumes:
      - ./app:/app
      - ./data:/data:ro
    depends_on: [spark-master, spark-worker-1, spark-worker-2]
    networks: [batchnet]

  postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d airflow"]
      interval: 5s
      timeout: 5s
      retries: 10
    volumes:
      - pgdata:/var/lib/postgresql/data

  # airflow-webserver:
  #   build:
  #     context: .
  #     dockerfile: ./compose/local/airflow/Dockerfile
  #   environment:
  #     AIRFLOW__CORE__EXECUTOR: LocalExecutor
  #     AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
  #     AIRFLOW__CORE__LOAD_EXAMPLES: "False"
  #     AIRFLOW__CORE__DEFAULT_TIMEZONE: America/Sao_Paulo
  #     # cria o usuário admin automaticamente
  #     _AIRFLOW_WWW_USER_CREATE: "true"
  #     _AIRFLOW_WWW_USER_USERNAME: admin
  #     _AIRFLOW_WWW_USER_PASSWORD: admin
  #     _AIRFLOW_WWW_USER_FIRSTNAME: Admin
  #     _AIRFLOW_WWW_USER_LASTNAME: User
  #     _AIRFLOW_WWW_USER_EMAIL: admin@example.com
  #     # ES vars para suas DAGs
  #     ES_HOST: ${ES_HOST}
  #     ES_USER: ${ES_USER}
  #     ES_PASSWORD: ${ES_PASSWORD}
  #     ES_INDEX: ${ES_INDEX:-openalex}
  #     ES_PIPELINE: ${ES_PIPELINE:-openalex}
  #   command: >
  #     bash -lc "
  #       airflow db migrate &&
  #       exec airflow webserver
  #     "
  #   ports: ["8083:8080"]
  #   depends_on:
  #     postgres:
  #       condition: service_healthy
  #   volumes:
  #     - ./dags:/opt/airflow/dags
  #     - ./logs:/opt/airflow/logs
  #     - ./plugins:/opt/airflow/plugins
  #   restart: unless-stopped

  # airflow-scheduler:
  #   build: 
  #     context: .
  #     dockerfile: ./compose/local/airflow/Dockerfile
  #   environment:
  #     AIRFLOW__CORE__EXECUTOR: LocalExecutor
  #     AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
  #   command: >
  #     bash -lc "
  #       airflow db migrate &&
  #       exec airflow scheduler
  #     "
  #   depends_on:
  #     postgres:
  #       condition: service_healthy
  #   volumes:
  #     - ./dags:/opt/airflow/dags
  #     - ./logs:/opt/airflow/logs
  #     - ./plugins:/opt/airflow/plugins
  #   restart: unless-stopped

volumes:
  osdata:
  pgdata:

networks:
  batchnet:
